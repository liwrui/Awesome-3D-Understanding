# Awesome 3D Scene Understanding
This repo collects papers, docs, codes about 3D Scene Understanding for anyone who wants to do research on it. We are continuously improving the project. Welcome to PR the works (papers, repositories) that are missed by the repo. Special thanks to [Fucheng Cai](https://github.com/HITCai), [Zhe yang](https://github.com/xia-zhe), and all researchers who have contributed to this project!

## Table of Contents

- [Papers](#Papers)
  - [2024](#2024)
  - [2023](#2023)
  - [2022](#2022)
  - [2021](#2021)


## Papers

### 2024
- [[ArXiv](https://arxiv.org/abs/2405.05258)] Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving [[code](https://github.com/ldkong1205/LaserMix)]
- [[AAAI](https://arxiv.org/abs/2306.02329)] SQA3D: Situated Question Answering in 3D Scenes [[code](https://sqa3d.github.io/)]
- [[AAAI](https://arxiv.org/abs/2402.15933)] Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA [[code](https://github.com/matthewdm0816/BridgeQA)]
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2024/html/Zheng_Towards_Learning_a_Generalist_Model_for_Embodied_Navigation_CVPR_2024_paper.html)] Towards Learning a Generalist Model for Embodied Navigation [[code](https://github.com/LaVi-Lab/NaviLLM)]

  
### 2023
- [[CVPRW](https://openaccess.thecvf.com/content/CVPR2023W/O-DRUM/html/Parelli_CLIP-Guided_Vision-Language_Pre-Training_for_Question_Answering_in_3D_Scenes_CVPRW_2023_paper.html)] CLIP-Guided Vision-Language Pre-Training for Question Answering in 3D Scenes [[code](https://github.com/alexdelitzas/3d-vqa)]
### 2022
- [[CVPR](https://openaccess.thecvf.com/content/CVPR2022/html/Azuma_ScanQA_3D_Question_Answering_for_Spatial_Scene_Understanding_CVPR_2022_paper.html)] ScanQA: 3D Question Answering for Spatial Scene Understanding [[code](https://github.com/ATR-DBI/ScanQA)]
### 2021
  
